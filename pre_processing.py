# -*- coding: utf-8 -*-
"""pre_processing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JzK_m-tKBPGlHtRGtQgqSM9CmmI5bjYA
"""

import pandas as pd
import numpy as np
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Initialize VADER sentiment analyzer
analyzer = SentimentIntensityAnalyzer()

# Reference expected sentiment scores for validation
REFERENCE_SENTIMENTS = {
    "03/25/2025": {
        "News_Sentiment_Avg": 0.2708,
        "Social_Sentiment_Avg": 0.2131
    }
}

# Function to compute sentiment score for a text
def get_sentiment(text):
    try:
        if not isinstance(text, str) or text.strip() == "" or text.lower() == "n/a":
            logger.warning(f"Empty or invalid text: {text}")
            return 0.0
        score = analyzer.polarity_scores(text.strip())["compound"]
        rounded_score = round(score, 4)
        logger.info(f"Text: {text[:50]}... Sentiment: {rounded_score}")
        return rounded_score
    except Exception as e:
        logger.error(f"Error computing sentiment for text: {text[:50]}... Error: {e}")
        return 0.0

# Function to standardize date format to MM/DD/YYYY
def standardize_date(date_str):
    try:
        if isinstance(date_str, str):
            date_str = date_str.strip()
            parsed_date = pd.to_datetime(date_str, errors='coerce')
            if pd.isna(parsed_date):
                logger.warning(f"Invalid date format: {date_str}")
                return "01/01/1970"
            return parsed_date.strftime("%m/%d/%Y")
        return date_str
    except Exception as e:
        logger.error(f"Error parsing date {date_str}: {e}")
        return "01/01/1970"

# Function to validate and clean numerical columns
def clean_numerical_column(series, column_name, expected_values=None):
    try:
        series_str = series.astype(str).str.strip()
        invalid_entries = series_str[~series_str.str.replace('-', '').str.replace('.', '').str.isnumeric()]
        if not invalid_entries.empty:
            logger.warning(f"Invalid entries in {column_name}:\n{invalid_entries}")
            for idx in invalid_entries.index:
                if expected_values and series_str[idx] in expected_values:
                    series[idx] = expected_values[series_str[idx]]
                else:
                    logger.warning(f"Replacing invalid {column_name} at index {idx}: {series[idx]} with 0")
                    series[idx] = 0
        return pd.to_numeric(series, errors='coerce').fillna(0)
    except Exception as e:
        logger.error(f"Error cleaning {column_name}: {e}")
        return series

# Function to normalize numerical columns using min-max scaling
def min_max_normalize(series):
    try:
        min_val = series.min()
        max_val = series.max()
        if max_val == min_val:
            logger.warning(f"No variation in series: {series.name}, returning zeros")
            return pd.Series([0.0] * len(series), index=series.index)
        normalized = (series - min_val) / (max_val - min_val)
        return normalized.round(4)
    except Exception as e:
        logger.error(f"Error normalizing series {series.name}: {e}")
        return pd.Series([0.0] * len(series), index=series.index)

# Main preprocessing function
def preprocess_data(trades_file, portfolio_file, news_file, social_file, fear_greed_file, output_file):
    try:
        # Load datasets
        logger.info("Loading datasets...")
        trades_df = pd.read_csv(trades_file)
        portfolio_df = pd.read_csv(portfolio_file)
        news_df = pd.read_csv(news_file)
        social_df = pd.read_csv(social_file)
        fear_greed_df = pd.read_csv(fear_greed_file)

        # Standardize date formats to MM/DD/YYYY
        logger.info("Standardizing date formats...")
        portfolio_df["Date"] = portfolio_df["Date"].apply(standardize_date)
        trades_df["Date"] = trades_df["Date"].apply(standardize_date)
        news_df["Date"] = news_df["Date"].apply(standardize_date)
        social_df["Date"] = social_df["Date"].apply(standardize_date)
        fear_greed_df["Date"] = fear_greed_df["Date"].apply(standardize_date)

        # Validate date standardization
        for df, name in [(portfolio_df, "portfolio"), (trades_df, "trades"),
                        (news_df, "news"), (social_df, "social"), (fear_greed_df, "fear_greed")]:
            invalid_dates = df[df["Date"] == "01/01/1970"]
            if not invalid_dates.empty:
                logger.warning(f"Invalid dates found in {name}.csv:\n{invalid_dates[['Date']]}")

        # Validate and clean numerical columns in trades_df
        logger.info("Validating numerical columns in trades.csv...")
        trades_df["PnL$"] = clean_numerical_column(trades_df["PnL$"], "PnL$", expected_values={"100 frivolity.00": 100.00})
        trades_df["R/Factor"] = clean_numerical_column(trades_df["R/Factor"], "R/Factor")

        # Validate trades_df
        logger.info("Validating trades.csv...")
        logger.info(f"Total trades: {len(trades_df)}")
        logger.info(f"Unique Trade IDs: {trades_df['TradeID'].nunique()}")
        if trades_df['TradeID'].duplicated().any():
            logger.warning("Duplicate Trade IDs found:")
            logger.warning(trades_df[trades_df['TradeID'].duplicated(keep=False)][['TradeID', 'Date', 'PnL$', 'R/Factor']])
        trade_counts = trades_df["Date"].value_counts().sort_index()
        logger.info("Trade counts per date:\n" + trade_counts.to_string())

        # Aggregate trades by Date
        logger.info("Aggregating trade data...")
        trades_agg = trades_df.groupby("Date", as_index=False).agg(
            Total_PnL=("PnL$", lambda x: np.sum(x.astype(float))),
            Win_Count=("PnL$", lambda x: np.sum(x.astype(float) > 0)),
            Loss_Count=("PnL$", lambda x: np.sum(x.astype(float) < 0)),
            Avg_R_Factor=("R/Factor", lambda x: np.round(np.mean(x.astype(float)), 4)),
            Trade_Count=("TradeID", "count")
        )
        logger.info("Aggregated trade data:\n" + trades_agg[["Date", "Total_PnL", "Win_Count", "Loss_Count", "Avg_R_Factor", "Trade_Count"]].to_string(index=False))

        # Validate trade aggregation
        logger.info("Validating trade aggregation...")
        for date in trades_df["Date"].unique():
            trades_on_date = trades_df[trades_df["Date"] == date]
            if not trades_on_date.empty:
                logger.info(f"Validating aggregation for {date}:")
                expected_pnl = trades_on_date['PnL$'].astype(float).sum()
                expected_wins = (trades_on_date['PnL$'].astype(float) > 0).sum()
                expected_losses = (trades_on_date['PnL$'].astype(float) < 0).sum()
                expected_r_factor = trades_on_date['R/Factor'].astype(float).mean()
                agg_row = trades_agg[trades_agg["Date"] == date]
                if not agg_row.empty:
                    if not (
                        abs(agg_row['Total_PnL'].iloc[0] - expected_pnl) < 1e-6 and
                        agg_row['Win_Count'].iloc[0] == expected_wins and
                        agg_row['Loss_Count'].iloc[0] == expected_losses and
                        abs(agg_row['Avg_R_Factor'].iloc[0] - expected_r_factor) < 1e-4
                    ):
                        logger.warning(f"Aggregation mismatch for {date}: Expected {expected_pnl}, {expected_wins}, {expected_losses}, {expected_r_factor}; Got {agg_row['Total_PnL'].iloc[0]}, {agg_row['Win_Count'].iloc[0]}, {agg_row['Loss_Count'].iloc[0]}, {agg_row['Avg_R_Factor'].iloc[0]}")

        # Sentiment analysis for news
        logger.info("Computing news sentiment...")
        news_df["Sentiment_1"] = news_df["Headline_1"].apply(get_sentiment)
        news_df["Sentiment_2"] = news_df["Headline_2"].apply(get_sentiment)
        news_df["Sentiment_3"] = news_df["Headline_3"].apply(get_sentiment)
        news_df["News_Sentiment_Avg"] = news_df[["Sentiment_1", "Sentiment_2", "Sentiment_3"]].mean(axis=1).round(4)
        logger.info("News sentiment (sample):\n" + news_df[["Date", "Headline_1", "Sentiment_1", "Headline_2", "Sentiment_2",
                                                           "Headline_3", "Sentiment_3", "News_Sentiment_Avg"]].head().to_string(index=False))

        # Validate news sentiment
        logger.info("Validating news sentiment...")
        for date, expected in REFERENCE_SENTIMENTS.items():
            if date in news_df["Date"].values:
                actual = news_df[news_df["Date"] == date]["News_Sentiment_Avg"].iloc[0]
                if abs(actual - expected["News_Sentiment_Avg"]) > 0.01:
                    logger.warning(f"News sentiment mismatch for {date}: Expected {expected['News_Sentiment_Avg']}, Got {actual}")

        # Sentiment analysis for social media
        logger.info("Computing social media sentiment...")
        social_df["Sentiment_1"] = social_df["Post_1"].apply(get_sentiment)
        social_df["Sentiment_2"] = social_df["Post_2"].apply(get_sentiment)
        social_df["Sentiment_3"] = social_df["Post_3"].apply(get_sentiment)
        social_df["Social_Sentiment_Avg"] = social_df[["Sentiment_1", "Sentiment_2", "Sentiment_3"]].mean(axis=1).round(4)
        logger.info("Social media sentiment (sample):\n" + social_df[["Date", "Post_1", "Sentiment_1", "Post_2", "Sentiment_2",
                                                                    "Post_3", "Sentiment_3", "Social_Sentiment_Avg"]].head().to_string(index=False))

        # Validate social media sentiment
        logger.info("Validating social media sentiment...")
        for date, expected in REFERENCE_SENTIMENTS.items():
            if date in social_df["Date"].values:
                actual = social_df[social_df["Date"] == date]["Social_Sentiment_Avg"].iloc[0]
                if abs(actual - expected["Social_Sentiment_Avg"]) > 0.01:
                    logger.warning(f"Social sentiment mismatch for {date}: Expected {expected['Social_Sentiment_Avg']}, Got {actual}")

        # Merge datasets
        logger.info("Merging datasets...")
        merged_df = portfolio_df[["Date", "USD_Balance", "BTC_Price", "ETH_Price", "Portfolio_Value_USD"]].merge(
            trades_agg[["Date", "Total_PnL", "Win_Count", "Loss_Count", "Avg_R_Factor"]],
            on="Date", how="left"
        ).merge(
            news_df[["Date", "News_Sentiment_Avg"]],
            on="Date", how="left"
        ).merge(
            social_df[["Date", "Social_Sentiment_Avg"]],
            on="Date", how="left"
        ).merge(
            fear_greed_df[["Date", "Index", "Sentiment"]],
            on="Date", how="left"
        )

        # Validate merge
        logger.info("Merged DataFrame info:")
        logger.info(merged_df.info())
        logger.info("Missing values:\n" + merged_df.isnull().sum().to_string())

        # Normalize numerical columns
        logger.info("Normalizing numerical columns...")
        numerical_columns = ["USD_Balance", "BTC_Price", "ETH_Price", "Portfolio_Value_USD",
                            "Total_PnL", "Win_Count", "Loss_Count", "Avg_R_Factor", "Index"]
        for col in numerical_columns:
            if col in merged_df.columns:
                merged_df[f"{col}_Normalized"] = min_max_normalize(merged_df[col])
                logger.info(f"Normalized {col}: min={merged_df[col].min()}, max={merged_df[col].max()}")

        # Save processed data
        logger.info(f"Saving processed data to {output_file}...")
        merged_df.to_csv(output_file, index=False)
        logger.info("Preprocessing complete!")

        return merged_df

    except Exception as e:
        logger.error(f"Error in preprocessing: {e}")
        raise

if __name__ == "__main__":
    # Example file paths (adjust as needed)
    trades_file = "trades.csv"
    portfolio_file = "portfolio.csv"
    news_file = "news.csv"
    social_file = "social_media.csv"
    fear_greed_file = "fear_greed.csv"
    output_file = "processed_data.csv"

    # Run preprocessing
    processed_df = preprocess_data(trades_file, portfolio_file, news_file, social_file, fear_greed_file, output_file)
    logger.info("Processed DataFrame (first 5 rows):\n" + processed_df.head().to_string())